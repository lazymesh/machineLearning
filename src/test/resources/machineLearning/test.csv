HashingTF is a Transformer which takes sets of terms and converts those sets into fixed-length feature vectors.
In text processing, a “set of terms” might be a bag of words. HashingTF utilizes the hashing trick.
A raw feature is mapped into an index (term) by applying a hash function. The hash function used here is MurmurHash 3.
Then term frequencies are calculated based on the mapped indices.
This approach avoids the need to compute a global term-to-index map, which can be expensive for a large corpus,
but it suffers from potential hash collisions, where different raw features may become the same term after hashing.
To reduce the chance of collision, we can increase the target feature dimension, i.e. the number of buckets of the hash table.
Since a simple modulo is used to transform the hash function to a column index, it is advisable to use a power of two as the
feature dimension, otherwise the features will not be mapped evenly to the columns. The default feature dimension is 218=262,144.
An optional binary toggle parameter controls term frequency counts. When set to true all nonzero frequency counts are set to 1.
This is especially useful for discrete probabilistic models that model binary, rather than integer, counts.